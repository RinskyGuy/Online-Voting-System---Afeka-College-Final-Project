{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e0a0418",
   "metadata": {},
   "source": [
    "# **Metrics**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27ccfa37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d3ee33",
   "metadata": {},
   "source": [
    "## **Metrics computation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa50c3e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(args):\n",
    "    assert len(args) == 4, f'Args must be float threshold or tps, fps, tns, fns values, but got {type(args)}.'\n",
    "    tps, fps, tns, fns = args\n",
    "    \n",
    "    if tps!=0 or fps!=0 or tns!=0 or fns!=0:\n",
    "        return (tps+tns)/(tps+tns+fps+fns)\n",
    "    return 0\n",
    "\n",
    "def get_precision(args):\n",
    "    assert len(args) == 4, f'Args must be float threshold or tps, fps, tns, fns values, but got {type(args)}.'\n",
    "    tps, fps, tns, fns = args\n",
    "    \n",
    "    if tps!=0 or fps!=0:\n",
    "        return tps/(tps+fps)\n",
    "    return 0\n",
    "\n",
    "def get_recall(args):\n",
    "    assert len(args) == 4, f'Args must be float threshold or tps, fps, tns, fns values, but got {type(args)}.'\n",
    "    tps, fps, tns, fns = args\n",
    "    if tps!=0 or fns!=0:\n",
    "        return tps/(tps+fns)\n",
    "    return 0\n",
    "\n",
    "def get_F1_score(args):\n",
    "    assert len(args) == 4, f'Args must be float threshold or tps, fps, tns, fns values, but got {type(args)}.'\n",
    "    tps, fps, tns, fns = args\n",
    "        \n",
    "    precision = get_precision(args)\n",
    "    recall = get_recall(args)\n",
    "    if precision!=0 or recall!=0:\n",
    "        return (2*precision*recall)/(precision+recall)\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19cf7d2c",
   "metadata": {},
   "source": [
    "## **Metrics for thresholds combinations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f742ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_thresh(tps_df, fps_df, fields, thresholds):\n",
    "    tps = tps_df\n",
    "    fps = fps_df\n",
    "    \n",
    "    for field, threshold in zip(fields, thresholds):\n",
    "        if threshold is not None:\n",
    "            tps = tps[tps[field] < threshold]\n",
    "            fps = fps[fps[field] < threshold]\n",
    "        \n",
    "    tps = len(tps)\n",
    "    fns = len(tps_df) - tps\n",
    "    \n",
    "    fps = len(fps)\n",
    "    tns = len(fps_df) - fps\n",
    "                        \n",
    "    return tps, fps, tns, fns\n",
    "\n",
    "def get_all_field_thresholds(field_mean, field_std, upper_bound=1.0):\n",
    "    thresholds = []\n",
    "    alpha = 0\n",
    "    \n",
    "    while True:\n",
    "        threshold = field_mean + alpha*field_std\n",
    "        \n",
    "        if len(thresholds) > 1 and threshold >= upper_bound:\n",
    "            break\n",
    "            \n",
    "        thresholds.append(threshold)\n",
    "        alpha += 1\n",
    "        \n",
    "    return thresholds\n",
    "        \n",
    "    \n",
    "def get_all_combinations(fields_thresholds):\n",
    "    return list(itertools.product(*fields_thresholds))\n",
    "\n",
    "def get_thresholds(fields, tps_df, fps_df):\n",
    "    assert isinstance(fields, list), f'Expected a list of fields data, but got {type(fields)}.'\n",
    "    assert len(fields)>0, \"Fields can't be null.\"\n",
    "    fields_names = []\n",
    "    fields_thresholds = []\n",
    "    \n",
    "    for field in fields:\n",
    "        assert isinstance(field, dict), f'Expected field data to be a dictionary, but got {type(field)}.'\n",
    "        assert 'name' in field, f'Expected \"name\" to be a field in data for field.'\n",
    "\n",
    "        upper_bound = fps_df[field['name']].mean()\n",
    "        if 'upper_bound' in field:\n",
    "            upper_bound = field['upper_bound']\n",
    "            assert isinstance(field['upper_bound'], float), f'Expected upper_bound of type float, but got {type(upper_bound)}.'\n",
    "            \n",
    "        fields_names.append(field['name'])\n",
    "        fields_thresholds.append(get_all_field_thresholds(tps_df[field['name']].mean(), tps_df[field['name']].std(), upper_bound=upper_bound))\n",
    "    \n",
    "    threshold_combinations = get_all_combinations(fields_thresholds)\n",
    "    \n",
    "    return fields_names, threshold_combinations\n",
    "\n",
    "def get_metrics(tps_df, fps_df, fields):\n",
    "    # fields - list of dicts with field name, mean and std\n",
    "    metrics = []        \n",
    "    fields_names, threshold_combinations = get_thresholds(fields, tps_df, fps_df)\n",
    "    \n",
    "    for fields_thresholds in threshold_combinations:\n",
    "        curr_metric = {}\n",
    "        \n",
    "        tps, fps, tns, fns = check_thresh(tps_df, fps_df, fields_names, fields_thresholds)\n",
    "        accuracy = get_accuracy((tps, fps, tns, fns))\n",
    "        precision = get_precision((tps, fps, tns, fns))\n",
    "        recall = get_recall((tps, fps, tns, fns))\n",
    "        f1_score = get_F1_score((tps, fps, tns, fns))\n",
    "\n",
    "        for field, threshold in zip(fields_names, fields_thresholds):\n",
    "            curr_metric[field+'_thresh'] = threshold\n",
    "\n",
    "        curr_metric.update({'%tp':round((tps/len(tps_df))*100, 4), \n",
    "                            '%fn':round((fns/len(tps_df))*100, 4), \n",
    "                            '%tn':round((tns/(len(fps_df)))*100, 4), \n",
    "                            '%fp':round((fps/(len(fps_df)))*100, 4), \n",
    "                            'accuracy':accuracy, \n",
    "                            'f1_score':f1_score, \n",
    "                            'precision':precision, \n",
    "                            'recall':recall})\n",
    "\n",
    "        metrics.append(curr_metric)\n",
    "    \n",
    "    return pd.DataFrame(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "40187793",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_thresholds(metrics_df, metrics, n=1, return_type='df'):\n",
    "    def get_field_name(col_name):\n",
    "        return '_'.join(col_name.split('_')[:-1])\n",
    "    \n",
    "    metrics_df = metrics_df.sort_values(by=metrics, ascending=False)\n",
    "    best_thresholds_df = metrics_df.iloc[:n]\n",
    "\n",
    "    if return_type == 'df':\n",
    "        return best_thresholds_df\n",
    "    \n",
    "    assert return_type == 'dict', f'Expected return_type to be an str of \"df\" or \"dict\", but got {return_type}.'\n",
    "    best_thresholds = []\n",
    "    for index, row in best_thresholds_df.iterrows():\n",
    "        best_threshold = {}\n",
    "        for col in best_thresholds_df.columns:\n",
    "            if col.endswith('thresh'):\n",
    "                best_threshold[get_field_name(col)] = row[col]\n",
    "        best_thresholds.append(best_threshold)\n",
    "        \n",
    "    return best_thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "11d98649",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_thresh(true_df, false_df, list_thresholds):\n",
    "    def is_below_thresholds(row, thresholds):\n",
    "        is_below = True\n",
    "        \n",
    "        for field, threshold in thresholds.items():\n",
    "            if row[field].values[0] > threshold:\n",
    "                is_below = False\n",
    "                \n",
    "        return is_below\n",
    "    \n",
    "    validation = []\n",
    "    for thresholds in list_thresholds:\n",
    "        tps, fps, tns, fns = 0, 0, 0, 0\n",
    "        row = 0\n",
    "        \n",
    "        while row<len(true_df) and row<len(false_df):\n",
    "            true_row = true_df.iloc[[row]]\n",
    "            false_row = false_df.iloc[[row]]\n",
    "            \n",
    "            if is_below_thresholds(true_row, thresholds):\n",
    "                tps+=1\n",
    "            else:\n",
    "                fns+=1\n",
    "\n",
    "\n",
    "            if is_below_thresholds(false_row, thresholds):\n",
    "                fps+=1\n",
    "            else:\n",
    "                tns+=1\n",
    "\n",
    "            row+=1\n",
    "            \n",
    "        while row<len(true_df):\n",
    "            true_row = true_df.iloc[[row]]\n",
    "            if is_below_thresholds(true_row, thresholds):\n",
    "                tps+=1\n",
    "            else:\n",
    "                fns+=1\n",
    "            \n",
    "            row+=1\n",
    "                \n",
    "        while row<len(false_df):\n",
    "            false_row = false_df.iloc[[row]]\n",
    "            if is_below_thresholds(false_row, thresholds):\n",
    "                fps+=1\n",
    "            else:\n",
    "                tns+=1\n",
    "            \n",
    "            row+=1\n",
    "\n",
    "        accuracy = get_accuracy((tps, fps, tns, fns))\n",
    "        precision = get_precision((tps, fps, tns, fns))\n",
    "        recall = get_recall((tps, fps, tns, fns))\n",
    "        f1_score = get_F1_score((tps, fps, tns, fns))\n",
    "        \n",
    "        curr_threshold = thresholds.copy()\n",
    "\n",
    "        curr_threshold.update({'tps%':tps/len(true_df)*100, 'fns%':fns/len(true_df)*100, \n",
    "                               'tns%':tns/len(false_df)*100, 'fps%':fps/len(false_df)*100,\n",
    "                               'accuracy':accuracy, 'f1_score':f1_score, 'precision':precision, 'recall':recall})\n",
    "        validation.append(curr_threshold)\n",
    "    \n",
    "    return pd.DataFrame(validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d02d53d",
   "metadata": {},
   "source": [
    "## **Plot distribution**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "195dc9eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(true, false, title='', cols=None):\n",
    "    def get_num_rows(cols, ncols):\n",
    "        if len(cols)<=ncols:\n",
    "            nrows=1\n",
    "        else:\n",
    "            nrows=len(cols)//ncols + len(cols)%ncols\n",
    "        \n",
    "        return nrows\n",
    "    \n",
    "    def plot_on_axes(true, false, axes, title):\n",
    "        ax = true[title].plot(kind=\"kde\", ax=axes, title=title, fontsize=20)\n",
    "        false[title].plot(kind=\"kde\", ax=ax, title=title, fontsize=20)\n",
    "    \n",
    "    if cols is None:\n",
    "        cols = true.columns\n",
    "    \n",
    "    ncols = 2\n",
    "    nrows = get_num_rows(cols, ncols)\n",
    "    fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(30,nrows*5))\n",
    "    fig.suptitle(title, fontsize=40)\n",
    "    plt.rcParams.update({'font.size': 20}) # must set in top\n",
    "    \n",
    "    nrow, ncol = 0, 0\n",
    "    for col in cols:\n",
    "        ax=axes[nrow,ncol] if nrows>1 else axes[ncol]\n",
    "        plot_on_axes(true, false, ax, col)\n",
    "        \n",
    "        ncol+=1\n",
    "        if ncol%ncols==0:\n",
    "            ncol=0\n",
    "            nrow+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60512eb2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda_env",
   "language": "python",
   "name": "cuda_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
